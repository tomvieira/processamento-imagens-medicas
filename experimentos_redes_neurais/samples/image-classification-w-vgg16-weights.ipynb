{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, we are going to use pretrained weights of VGG 16 and then add a new output layer with the required number of classes. In order to use the pretrained weights, you need add a new dataset containing the weights. Go to Data tab and click on 'Add Data Source'. Then search for 'Keras Pretrained Model' dataset which contains weights of different architectures like VGG16, Inception, Resnet50, Xception.\n\nThe dataset contains 3 directories: Training, Validation and Testing. Each directory contains sub-directories with images of different fruits.","metadata":{"_uuid":"da4cce1590dc1cac991d7562522ac8f16f6c5f48"}},{"cell_type":"code","source":"# importing the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom glob import glob","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the directories \ntraining_dir = '../input/fruits/fruits-360_dataset/fruits-360/Training/'\nvalidation_dir = '../input/fruits/fruits-360_dataset/fruits-360/Test/'\ntest_dir = '../input/fruits/fruits-360_dataset/fruits-360/test-multiple_fruits/'","metadata":{"_uuid":"593f6bf3064a52388f6f6dd2df6252980a631e6f","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# useful for getting number of files\nimage_files = glob(training_dir + '/*/*.jp*g')\nvalid_image_files = glob(validation_dir + '/*/*.jp*g')","metadata":{"_uuid":"2687d1c9f4a5d3aa631dfac1cd3c26fd0dd3d166","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting the number of classes i.e. type of fruits\nfolders = glob(training_dir + '/*')\nnum_classes = len(folders)\nprint ('Total Classes = ' + str(num_classes))","metadata":{"_uuid":"d9ecb46c4d6c9d2cf744b36cfced6398752383a4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this will copy the pretrained weights to our kernel\n!mkdir ~/.keras\n!mkdir ~/.keras/models\n!cp ../input/keras-pretrained-models/*notop* ~/.keras/models/\n!cp ../input/keras-pretrained-models/imagenet_class_index.json ~/.keras/models/","metadata":{"_uuid":"45d613325b9c0f27d67c44590aa191d67a91050f","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing the libraries\nfrom keras.models import Model\nfrom keras.layers import Flatten, Dense\nfrom keras.applications import VGG16\n#from keras.preprocessing import image\n\nIMAGE_SIZE = [64, 64]  # we will keep the image size as (64,64). You can increase the size for better results. \n\n# loading the weights of VGG16 without the top layer. These weights are trained on Imagenet dataset.\nvgg = VGG16(input_shape = IMAGE_SIZE + [3], weights = 'imagenet', include_top = False)  # input_shape = (64,64,3) as required by VGG\n\n# this will exclude the initial layers from training phase as there are already been trained.\nfor layer in vgg.layers:\n    layer.trainable = False\n\nx = Flatten()(vgg.output)\n#x = Dense(128, activation = 'relu')(x)   # we can add a new fully connected layer but it will increase the execution time.\nx = Dense(num_classes, activation = 'softmax')(x)  # adding the output layer with softmax function as this is a multi label classification problem.\n\nmodel = Model(inputs = vgg.input, outputs = x)\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"_uuid":"cfdd83abd9a203157499159e0e087c9530aeea1a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"_uuid":"703eb0b5beabb2508c5381b7f21d048f64712e8f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Image Augmentation\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import preprocess_input\n\ntraining_datagen = ImageDataGenerator(\n                                    rescale=1./255,   # all pixel values will be between 0 an 1\n                                    shear_range=0.2, \n                                    zoom_range=0.2,\n                                    horizontal_flip=True,\n                                    preprocessing_function=preprocess_input)\n\nvalidation_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n\ntraining_generator = training_datagen.flow_from_directory(training_dir, target_size = IMAGE_SIZE, batch_size = 200, class_mode = 'categorical')\nvalidation_generator = validation_datagen.flow_from_directory(validation_dir, target_size = IMAGE_SIZE, batch_size = 200, class_mode = 'categorical')","metadata":{"_uuid":"24bccf942ce0f39558cff2a04a18a6a30ede9998","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have not used any X_train, y_train, X_test, y_test or generated any labels for our classes. It is because we are using the flow_from_directory function of ImageDataGenerator. This function takes a directory as an input and assign the class indices to its sub directories. For this function to work, each subdirectory must contain a single class object only.\nAnother function of ImageDataGenerator is 'flow'. With this function, we need to provide X_train, y_train. If you use X_train, y_train, X_test, y_test, don't forget to normalize X_train and X_test and use on hot encoding for y_train and y_test.","metadata":{"_uuid":"571943a0ecb771740ceba5fe198f936a8e311fb0"}},{"cell_type":"code","source":"# The labels are stored in class_indices in dictionary form. \n# checking the labels\ntraining_generator.class_indices","metadata":{"_uuid":"96302bd261bd293fe19b02ae88f2ab5ddaa75d07","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_images = 37836\nvalidation_images = 12709\n\nhistory = model.fit_generator(training_generator,\n                   steps_per_epoch = 10000,  # this should be equal to total number of images in training set. But to speed up the execution, I am only using 10000 images. Change this for better results. \n                   epochs = 1,  # change this for better results\n                   validation_data = validation_generator,\n                   validation_steps = 3000)  # this should be equal to total number of images in validation set.","metadata":{"scrolled":true,"_uuid":"ffb1b7ce1f7560753f83a10de47cf290fbec9159","_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print ('Training Accuracy = ' + str(history.history['acc']))\nprint ('Validation Accuracy = ' + str(history.history['val_acc']))","metadata":{"_uuid":"c147c541d4c6d71d03cf2011c6933486b05db1da","trusted":true},"execution_count":null,"outputs":[]}]}